\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\optimization}[2]{
	\[
		\begin{array}{rcl}
			#1 & \Rightarrow & #2
		\end{array}
	\]
}

\title{G2C: an optimizing transcompiler for probabilistic programming languages}
\author{Robert Brignull}
\date{ }

\begin{document}

\maketitle

%    #### ##    ## ######## ######    ##### 
%     ##  ###   ##    ##    ##   ##  ##   ##
%     ##  ####  ##    ##    ##   ##  ##   ##
%     ##  ## ## ##    ##    ######   ##   ##
%     ##  ##  ####    ##    ##  ##   ##   ##
%     ##  ##   ###    ##    ##   ##  ##   ##
%    #### ##    ##    ##    ##    ##  ##### 

\section{Introduction}

\subsection{The purpose of the project}

Talk about how there a many different probabilistic programming languages.

A lot are both functional and interpreted as this makes them easy to code in and easy to implement, but it lessens their performance.

Prob-C is the opposite, being based on C it is more difficult to code in but by being a compiled language and highly multi-threaded it has good performance.

The purpose of this project is to create a transcompiler from a functional probabilistic programming language into Prob-C.

If any optimizations can be done along the way, either probabilistic or not, then that is even better.



\subsection{How this was achieved}

Because both my source language and Prob-C handle sampling and observing in a very similar way, the bulk of the compiler is very standard.

It first transforms into continuation passing style, then adds closures to functions and hoists them to the top level, before outputting C code.

It performs some non-probabilistic optimizations such as identifier reassignment and constant expression calculation.

It performs probabilistic optimizations such as merging multiple samples from one distribution family into one sample, and potentially removing observes when it forms a conjugate prior.



%    ######     ##     #####  ##   ##  #####   ######    #####  ##   ## ##    ## ###### 
%    ##   ##   ####   ##   ## ##  ##  ##   ##  ##   ##  ##   ## ##   ## ###   ## ##   ##
%    ##   ##  ##  ##  ##      ## ##   ##       ##   ##  ##   ## ##   ## ####  ## ##   ##
%    ######  ##    ## ##      ####    ##       ######   ##   ## ##   ## ## ## ## ##   ##
%    ##   ## ######## ##      ## ##   ##  #### ##  ##   ##   ## ##   ## ##  #### ##   ##
%    ##   ## ##    ## ##   ## ##  ##  ##   ##  ##   ##  ##   ## ##   ## ##   ### ##   ##
%    ######  ##    ##  #####  ##   ##  #####   ##    ##  #####   #####  ##    ## ###### 

\section{Background}

\subsection{Probabilistic programming}

A probabilistic programming language is just like a normal one except with the ability to easily draw from random distributions and to condition the program execution on other random variables.

Usually a probabilistic programming language is based on an existing language rather than being a completely new one, examples include: Venture (Scheme), Anglican (Scheme), IBAL (OCaml), Infer.NET (.NET), PSQL (SQL), FACTORIE (Scala), Alchemy (C++), Church (Scheme), Stan (none).

Probabilistic programming languages do their inference in one of a few different ways, including Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) algorithms.

The applications of probabilistic programming are diverse, anything that requires estimating a distribution given prior beliefs and conditioned on some observed data. It can be used in machine learning, financial modeling, etc.



\subsection{Anglican and Prob-c}

Anglican and Prob-C are the two languages we'll be concerned with most, Anglican because it's what I've based my source language on, and Prob-C because it is the target language of my compiler.

Anglican is based on the Venture modeling language which is itself based on Scheme. Anglican is effectively one language embedded inside another, specifically it is build from a sequence of the commands Assume, Observe and Predict, each of which can contain expressions which come from a functional subset of Scheme.

Prob-C on the other hand is based on full C rather than a subset, in fact the only difference is the addition of a library of sampling functions, two extra functions to observe and predict, and a macro which redefines the main function. So Prob-C has all the power of C. Unlike almost all other probabilistic programming languages, Prob-C is unsafe in the sense that it is possible to create programs which make no sense statistically, for this reason as well as C's unfriendliness it is not recommended to code in Prob-C directly.



\subsection{Bayesian probability}

Bayesian probability revolves around the idea of conditional probability, that is having some prior belief about an event and then estimating it's probability once you've observed that some other event happened.

The central theorem is Bayes' theorem \(p(A | B) = \dfrac{p(A \land B)}{p(B)}\)



%     #####   #####  ##      ## ######  #### ##     ####### ######  
%    ##   ## ##   ## ###    ### ##   ##  ##  ##     ##      ##   ## 
%    ##      ##   ## ####  #### ##   ##  ##  ##     ##      ##   ## 
%    ##      ##   ## ## #### ## ######   ##  ##     ######  ######  
%    ##      ##   ## ##  ##  ## ##       ##  ##     ##      ##  ##  
%    ##   ## ##   ## ##      ## ##       ##  ##     ##      ##   ## 
%     #####   #####  ##      ## ##      #### ###### ####### ##    ##

\section{The compiler}

\subsection{The source language}

For this project I had to invent my own toy language, I chose to base it on Anglican but with some extra type information to make compilation easier.

** Small example program **

Without this extra type information, when translated to C all variables would have to be data-structures rather than native types, this would largely eliminate the benefit of transforming to a compiled language.

** BNF of G **



\subsection{Typing the source language}

Typing the source language is very easy, all the information is there and only minimal inference has to be done.

We know the types of simple values such as numbers and booleans.

For all built-in functions (eg. +, -, and, or) we know their type or in the case of = can give them one based on their argument types. Then we just check that the types of the arguments match the types of the formal parameters.

For if expressions we check that the test is a boolean type and that both of the branches have the same type.

For lambdas the user must specify the parameter types and return type, so we just check that the actual type of the body matches the return type they gave. We have to do a little bit of extra work to deal with recursive functions, this is the reason why function return types must be specified.

For an observe, check that the outer expression of the first argument is a probabilistic primitive. This is a restriction of the sampling method and hence of the language.



\subsection{Making ids unique}

At this point we change all ids to be unique, that is so that whenever two ids are the same then they refer to the same variable.

This means that for the rest of the compilation we do not have to worry about the scope of variables at all.



\subsection{Into continuation passing style}

The first step is to transform into continuation passing style.

The main difference is that now functions do not return values but instead take as another parameter a function to call when they're done with the value they would have returned.

The exception to this is built-in primitives which we assume can be done atomically by the target language and hence they can be calculated directly and stored into variables.

It is this step that is the defining transition from a functional to an imperative language.



\subsection{Closure conversion and hoisting}

What we want to do in this step is to lift all functions to the top level of the program, as they must be defined that way in a C program. The problem is that functions may reference variables defined outside of their scope.

Closure conversion concerns working out which are the variables used by a function are free variables and which are present in the arguments or defined by let expressions within that function.

The effect of this is almost to add more arguments to the function so that it has no free variables, in practice however we keep separate the original arguments and the new arguments are formed into one package, a closure or bundle.

Once this has been done all function definitions can be lifted to the top level of the program. Whenever a function would have been defined we instead instantiate a data structure containing the name of the function and the values of all variables it references formed into a closure.



\subsection{Outputting C code}

We need to output valid C code, including struct definitions and any extra library functions used.

Mostly the translation here is obvious and direct, only difficult point is to make sure all structs and functions are defined before they are described to allow full recursion.

One important point is we don't need to worry about memory management at all, which simplifies like tremendously. This is because each run of the program happens within its own thread and when it ends the operating system will release all memory it owns. Because each thread is short lived and doesn't use much memory individually, the memory usage of the program is low and does not grow over time.



%     #####  ######  ######## #### ##      ## #### ########    ##    ######## ####  #####  ##    ##  ##### 
%    ##   ## ##   ##    ##     ##  ###    ###  ##       ##    ####      ##     ##  ##   ## ###   ## ##     
%    ##   ## ##   ##    ##     ##  ####  ####  ##      ##    ##  ##     ##     ##  ##   ## ####  ## ##     
%    ##   ## ######     ##     ##  ## #### ##  ##     ##    ##    ##    ##     ##  ##   ## ## ## ##  ##### 
%    ##   ## ##         ##     ##  ##  ##  ##  ##    ##     ########    ##     ##  ##   ## ##  ####      ##
%    ##   ## ##         ##     ##  ##      ##  ##   ##      ##    ##    ##     ##  ##   ## ##   ###      ##
%     #####  ##         ##    #### ##      ## #### ######## ##    ##    ##    ####  #####  ##    ##  ##### 

\section{Optimizations}

Although just having a compiler from a high level probabilistic language into a lower lever one such as Prob-C would be a good enough project in itself, there is potential for massive increases in both performance and the quality of samples though a combination of a few simple optimizations.

All the optimizations described here are performed immediately after CPS transformation. In this stage the syntax is in a good balance of simple and expressive, for example all expressions have been split up so we only calculate one primitive or application at a time, but lambdas are still in-line and so we don't have to worry about variable scope as much. It is however still often useful to know the full history of a variable, that is exactly how it was calculated rather than just the last step, so we calculate this history before doing any optimizations and rebuild the information after each one.



\subsection{Non-probabilistic optimizations}

We do some standard non-probabilistic optimizations. None of these are technically necessary for the rest of the optimizations to work and some would be performed by the C compiler anyway. They still make our life a little easier however so that is why they are included here.



\subsubsection{Constant expression calculation}

When the arguments to a primitive such as plus or times are all constant, we can easily calculate what the answer should be and set the id equal to that rather than applying a primitive.

For example we could do the following optimization

\optimization{
	\begin{array}{l}
		\texttt{let x = prim + 2 5 in} \\
		\texttt{...}
	\end{array}
}{
	\begin{array}{l}
		\texttt{let x = 7 in} \\
		\texttt{...}
	\end{array}
}

This can be extended to the case when only some of the arguments are constant, now we can't remove the whole primitive application but we can reduce the number of arguments and hence the complexity of it.

For example we could do the following optimization

\optimization{
	\begin{array}{l}
		\texttt{let x = ... in} \\
		\texttt{let y = prim + x 2 5 in} \\
		\texttt{...}
	\end{array}
}{
	\begin{array}{l}
		\texttt{let x = ... in} \\
		\texttt{let y = prim + x 7 in} \\
		\texttt{...}
	\end{array}
}

We can optimize all of the following when at least some of the arguments are constant:
\begin{center}
	\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, \texttt{eq}, \texttt{neq}, \texttt{<}, \texttt{>}, \texttt{<=}, \texttt{>=}, \texttt{and}, \texttt{or}, \texttt{not}
\end{center}

This sort of optimization would definitely be performed by any half decent C compiler, so we are not actually improving the performance of the final compiled program by doing this. The reason for doing it is it reduces the size and complexity of the generated Prob-C code greatly, therefore making other stages of compilation faster and easier and also making it easier for a human to scan and understand the outputted Prob-C code.



\subsubsection{Merging multiple arithmetic expressions}

Another thing we can do with arithmetic expressions is to merge ones of the same sort together.

For example we could do the following optimization

\optimization{
	\begin{array}{l}
		\texttt{let x = ... in} \\
		\texttt{let y = prim + x 5 in} \\
		\texttt{let z = prim + y 8 in} \\
		\texttt{let w = prim + z -4 in} \\
		\texttt{...}
	\end{array}
}{
	\begin{array}{l}
		\texttt{let x = ... in} \\
		\texttt{let w = prim + x 9 in} \\
		\texttt{...}
	\end{array}
}

This is a case that comes up in compiled programs surprisingly often, especially after other optimizations, so by optimizing it away we reduce the final complexity of the Prob-C.

Again this optimization would likely be performed by the C compiler anyway so the final compiled program is unchanged.



\subsubsection{Removing let expressions where the id is not used}

A very simple optimization is to remove any let expression where the variable being assigned to is never used later on in the program.

This optimization doesn't do much on its own, as the user is unlikely to have written a program that doesn't use one of its variables, but almost all of the other optimizations we do here will leave orphaned variables that this process will clean up.

This is an optimization that a C compiler potentially would not perform. This is because C is a language with side effects so just because a variable is never used does not mean that the process of calculating it did not change program state. In our language however variables are immutable and there is no state so it is safe to remove the unused variable.



\subsubsection{Removing let expressions where an id is assigned to another id}

Another very simple optimization is to remove a variable when it is assigned to the value of another variable. We then change all references to the removed variable in the rest of the program to the variable it was assigned to.

For example we could do the following optimization

\optimization{
	\begin{array}{l}
		\texttt{let x = ... in} \\
		\texttt{let y = x in} \\
		\texttt{let z = prim + y 1 in}\\
		\texttt{...}
	\end{array}
}{
	\begin{array}{l}
		\texttt{let x = ... in} \\
		\texttt{let z = prim + x 1 in}\\
		\texttt{...}
	\end{array}
}

This optimization is admittedly rather inconsequential, but we include it anyway.



\subsubsection{Removing trivial continuations}

The final non-probabilistic optimization we do it to remove trivial functions which are equivalent to another function. The place where this happens a lot is continuations generated from the translation into CPS.

For example we could do the following optimization

\optimization{
	\begin{array}{l}
		\texttt{let f = lambda x y z -> g x y z in} \\
		\texttt{...}
	\end{array}
}{
	\begin{array}{l}
		\texttt{let f = g in} \\
		\texttt{...}
	\end{array}
}

This will then of course be removed on the next pass as it is an id assigned to another id.

This optimization could be moderately important for performance. Although a decent C compiler will inline a small function such as the above and hence remove the extra application, after the other transformations that G2C performs it will not be simple, so removing it at this stage will greatly simplify later stages of compilation.



\subsection{Merging of samples}

If we have the situation

\[
	\begin{array}{l}
		\lbrack \mathsf{assume}\ x_1\ (\mathsf{normal}\ m_1\ b_1) \rbrack \\
		\dots \\
		\lbrack \mathsf{assume}\ x_n\ (\mathsf{normal}\ m_n\ b_n) \rbrack \\
		\lbrack \mathsf{assume}\ y\ (+\ x_1\ ...\ x_n) \rbrack
	\end{array}
\]

and each \(x_i\) is not used anywhere else, then we can simplify it to just

\[\lbrack \mathsf{assume}\ y\ (\mathsf{normal}\ (+\ m_1\ \dots m_n)\ (+\ b_1 \dots b_n)) \rbrack\]

Similar things can be done with other distributions.



\subsection{Merging of observe statements}

\subsubsection{Merging observes of the same distribution}

If we have the situation

\[
	\begin{array}{l}
		\lbrack \mathsf{observe}\ (\mathsf{normal}\ m_1\ b_1)\ x_1 \rbrack \\
		\dots \\
		\lbrack \mathsf{observe}\ (\mathsf{normal}\ m_n\ b_n)\ x_n \rbrack
	\end{array}
\]

then it is equivalent to an observe that only relies on the sample mean and variance of the \(x_i\)s.

Note the resulting observe will not follow a normal distribution.



\subsubsection{Merging any consecutive observes}

The way that Prob-C observes work is that you call the observe function with the log of the probability of the event happening.

Also note that the distributions used for each observe are independent of each other.

Therefore the following

\[
	\begin{array}{l}
		\mathsf{observe}(\mathsf{dist}_1(args_1 ...)); \\
		\dots \\
		\mathsf{observe}(\mathsf{dist}_n(args_n ...));
	\end{array}
\]

is equivalent to

\[\mathsf{observe}(\sum_{i=1}^n\mathsf{dist}_i(args_i ...));\]



\subsection{Removal of observe statements}

\subsection{Commuting samples and observe statements}

Here we start doing some actual probability in an attempt to swap sample and observe statements and hopefully remove the observe completely.

The term for what we're doing is conjugate prior.

If we have the situation

\[
	\begin{array}{l}
		\lbrack \mathsf{assume}\ p\ (\mathsf{beta}\ a\ b) \rbrack \\
		\lbrack \mathsf{observe}\ (\mathsf{flip}\ p)\ \mathsf{true} \rbrack
	\end{array}
\]

then it can be simplified to

\[\lbrack \mathsf{assume}\ p\ (\mathsf{beta}\ (+\ a\ 1)\ b) \rbrack\]

This is the easy case where \(a\) and \(b\) are constants, if they are not then the observe remains, but we might reduce 100 observes to just one so it will hopefully improve performance and definitely improve the quality of the samples generated.



\subsection{Removal of constant observe statements}

When the arguments to an observe statement are all constant, then that observe can have no influence on the posterior distribution as all particles will observe the same value.

It is therefore safe to remove these constant observes.

\end{document}
